{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Supervised Learning\n",
    "### Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification vs Regression\n",
    "\n",
    "Your goal is to identify students who might need early intervention - which type of supervised machine learning problem is this, classification or regression? Why?\n",
    "\n",
    "This supervised machine learning problem belongs to classification, because the outcome should be two categorical values: need early intervention or not. And we need make decisions whether a student might need early intervention based on the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Data\n",
    "\n",
    "Let's go ahead and read in the student dataset first.\n",
    "\n",
    "_To execute a code cell, click inside it and press **Shift+Enter**._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print \"Student data read successfully!\"\n",
    "# Note: The last column 'passed' is the target/label, all other are feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you find out the following facts about the dataset?\n",
    "- Total number of students\n",
    "- Number of students who passed\n",
    "- Number of students who failed\n",
    "- Graduation rate of the class (%)\n",
    "- Number of features\n",
    "\n",
    "_Use the code block below to compute these values. Instructions/steps are marked using **TODO**s._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>passed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob     Fjob  \\\n",
       "0     GP   F   18       U     GT3       A     4     4  at_home  teacher   \n",
       "1     GP   F   17       U     GT3       T     1     1  at_home    other   \n",
       "2     GP   F   15       U     LE3       T     1     1  at_home    other   \n",
       "\n",
       "   ...   internet romantic  famrel  freetime  goout Dalc Walc health absences  \\\n",
       "0  ...         no       no       4         3      4    1    1      3        6   \n",
       "1  ...        yes       no       5         3      3    1    1      3        4   \n",
       "2  ...        yes       no       4         3      2    2    3      3       10   \n",
       "\n",
       "  passed  \n",
       "0     no  \n",
       "1     no  \n",
       "2    yes  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Number of features: 30\n",
      "Graduation rate of the class: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute desired values - replace each '?' with an appropriate expression/function call\n",
    "n_students = student_data.shape[0]\n",
    "n_features = student_data.shape[1] - 1\n",
    "n_passed = sum(student_data.passed=='yes')\n",
    "n_failed = sum(student_data.passed=='no')\n",
    "grad_rate = 100.0*n_passed/(n_passed + n_failed)\n",
    "print \"Total number of students: {}\".format(n_students)\n",
    "print \"Number of students who passed: {}\".format(n_passed)\n",
    "print \"Number of students who failed: {}\".format(n_failed)\n",
    "print \"Number of features: {}\".format(n_features)\n",
    "print \"Graduation rate of the class: {:.2f}%\".format(grad_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.\n",
    "\n",
    "Let's first separate our data into feature and target columns, and see if any features are non-numeric.<br/>\n",
    "**Note**: For this dataset, the last column (`'passed'`) is the target or label we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):-\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "Target column: passed\n",
      "\n",
      "Feature values:-\n",
      "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  \\\n",
      "0     GP   F   18       U     GT3       A     4     4  at_home   teacher   \n",
      "1     GP   F   17       U     GT3       T     1     1  at_home     other   \n",
      "2     GP   F   15       U     LE3       T     1     1  at_home     other   \n",
      "3     GP   F   15       U     GT3       T     4     2   health  services   \n",
      "4     GP   F   16       U     GT3       T     3     3    other     other   \n",
      "\n",
      "    ...    higher internet  romantic  famrel  freetime goout Dalc Walc health  \\\n",
      "0   ...       yes       no        no       4         3     4    1    1      3   \n",
      "1   ...       yes      yes        no       5         3     3    1    1      3   \n",
      "2   ...       yes      yes        no       4         3     2    2    3      3   \n",
      "3   ...       yes      yes       yes       3         2     2    1    1      5   \n",
      "4   ...       yes       no        no       4         3     2    1    2      5   \n",
      "\n",
      "  absences  \n",
      "0        6  \n",
      "1        4  \n",
      "2       10  \n",
      "3        2  \n",
      "4        4  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(student_data.columns[:-1])  # all columns but last are features\n",
    "target_col = student_data.columns[-1]  # last column is the target/label\n",
    "print \"Feature column(s):-\\n{}\".format(feature_cols)\n",
    "print \"Target column: {}\".format(target_col)\n",
    "\n",
    "X_all = student_data[feature_cols]  # feature values for all students\n",
    "y_all = student_data[target_col]  # corresponding targets/labels\n",
    "print \"\\nFeature values:-\"\n",
    "print X_all.head()  # print the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess feature columns\n",
    "\n",
    "As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48):-\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess feature columns\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "        # Note: This should change the data type for yes/no columns to int\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  # e.g. 'school' => 'school_GP', 'school_MS'\n",
    "\n",
    "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
    "\n",
    "    return outX\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print \"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "So far, we have converted all _categorical_ features into numeric values. In this next step, we split the data (both features and corresponding labels) into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_GP</th>\n",
       "      <th>school_MS</th>\n",
       "      <th>sex_F</th>\n",
       "      <th>sex_M</th>\n",
       "      <th>age</th>\n",
       "      <th>address_R</th>\n",
       "      <th>address_U</th>\n",
       "      <th>famsize_GT3</th>\n",
       "      <th>famsize_LE3</th>\n",
       "      <th>Pstatus_A</th>\n",
       "      <th>...</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   school_GP  school_MS  sex_F  sex_M  age  address_R  address_U  famsize_GT3  \\\n",
       "0          1          0      1      0   18          0          1            1   \n",
       "1          1          0      1      0   17          0          1            1   \n",
       "2          1          0      1      0   15          0          1            0   \n",
       "3          1          0      1      0   15          0          1            1   \n",
       "4          1          0      1      0   16          0          1            1   \n",
       "\n",
       "   famsize_LE3  Pstatus_A    ...     higher  internet  romantic  famrel  \\\n",
       "0            0          1    ...          1         0         0       4   \n",
       "1            0          0    ...          1         1         0       5   \n",
       "2            1          0    ...          1         1         0       4   \n",
       "3            0          0    ...          1         1         1       3   \n",
       "4            0          0    ...          1         0         0       4   \n",
       "\n",
       "   freetime  goout  Dalc  Walc  health  absences  \n",
       "0         3      4     1     1       3         6  \n",
       "1         3      3     1     1       3         4  \n",
       "2         3      2     2     3       3        10  \n",
       "3         2      2     1     1       5         2  \n",
       "4         3      2     1     2       5         4  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395, 48)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 300 samples\n",
      "Test set: 95 samples\n"
     ]
    }
   ],
   "source": [
    "# First, decide how many training vs test samples you want\n",
    "num_all = student_data.shape[0]  # same as len(student_data)\n",
    "num_train = 300  # about 75% of the data\n",
    "num_test = num_all - num_train\n",
    "\n",
    "# TODO: Then, select features (X) and corresponding labels (y) for the training and test sets\n",
    "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
    "np.random.seed(1)\n",
    "indices = np.random.permutation(num_all)\n",
    "train_index = indices[:num_train]\n",
    "test_index = indices[:-num_train]\n",
    "X_train = X_all.loc[train_index]\n",
    "y_train = y_all.loc[train_index]\n",
    "X_test = X_all.loc[test_index]\n",
    "y_test = y_all.loc[test_index]\n",
    "print \"Training set: {} samples\".format(X_train.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test.shape[0])\n",
    "# Note: If you need a validation set, extract it from within training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluating Models\n",
    "Choose 3 supervised learning models that are available in scikit-learn, and appropriate for this problem. For each model:\n",
    "\n",
    "- What are the general applications of this model? What are its strengths and weaknesses?\n",
    "- Given what you know about the data so far, why did you choose this model to apply?\n",
    "- Fit this model to the training data, try to predict labels (for both training and test sets), and measure the F<sub>1</sub> score. Repeat this process with different training set sizes (100, 200, 300), keeping test set constant.\n",
    "\n",
    "Produce a table showing training time, prediction time, F<sub>1</sub> score on training set and F<sub>1</sub> score on test set, for each training set size.\n",
    "\n",
    "Note: You need to produce 3 such tables - one for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_GP</th>\n",
       "      <th>school_MS</th>\n",
       "      <th>sex_F</th>\n",
       "      <th>sex_M</th>\n",
       "      <th>age</th>\n",
       "      <th>address_R</th>\n",
       "      <th>address_U</th>\n",
       "      <th>famsize_GT3</th>\n",
       "      <th>famsize_LE3</th>\n",
       "      <th>Pstatus_A</th>\n",
       "      <th>...</th>\n",
       "      <th>higher</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.883544</td>\n",
       "      <td>0.116456</td>\n",
       "      <td>0.526582</td>\n",
       "      <td>0.473418</td>\n",
       "      <td>16.696203</td>\n",
       "      <td>0.222785</td>\n",
       "      <td>0.777215</td>\n",
       "      <td>0.711392</td>\n",
       "      <td>0.288608</td>\n",
       "      <td>0.103797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.949367</td>\n",
       "      <td>0.832911</td>\n",
       "      <td>0.334177</td>\n",
       "      <td>3.944304</td>\n",
       "      <td>3.235443</td>\n",
       "      <td>3.108861</td>\n",
       "      <td>1.481013</td>\n",
       "      <td>2.291139</td>\n",
       "      <td>3.554430</td>\n",
       "      <td>5.708861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.321177</td>\n",
       "      <td>0.321177</td>\n",
       "      <td>0.499926</td>\n",
       "      <td>0.499926</td>\n",
       "      <td>1.276043</td>\n",
       "      <td>0.416643</td>\n",
       "      <td>0.416643</td>\n",
       "      <td>0.453690</td>\n",
       "      <td>0.453690</td>\n",
       "      <td>0.305384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219525</td>\n",
       "      <td>0.373528</td>\n",
       "      <td>0.472300</td>\n",
       "      <td>0.896659</td>\n",
       "      <td>0.998862</td>\n",
       "      <td>1.113278</td>\n",
       "      <td>0.890741</td>\n",
       "      <td>1.287897</td>\n",
       "      <td>1.390303</td>\n",
       "      <td>8.003096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        school_GP   school_MS       sex_F       sex_M         age   address_R  \\\n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean     0.883544    0.116456    0.526582    0.473418   16.696203    0.222785   \n",
       "std      0.321177    0.321177    0.499926    0.499926    1.276043    0.416643   \n",
       "min      0.000000    0.000000    0.000000    0.000000   15.000000    0.000000   \n",
       "25%      1.000000    0.000000    0.000000    0.000000   16.000000    0.000000   \n",
       "50%      1.000000    0.000000    1.000000    0.000000   17.000000    0.000000   \n",
       "75%      1.000000    0.000000    1.000000    1.000000   18.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000   22.000000    1.000000   \n",
       "\n",
       "        address_U  famsize_GT3  famsize_LE3   Pstatus_A     ...      \\\n",
       "count  395.000000   395.000000   395.000000  395.000000     ...       \n",
       "mean     0.777215     0.711392     0.288608    0.103797     ...       \n",
       "std      0.416643     0.453690     0.453690    0.305384     ...       \n",
       "min      0.000000     0.000000     0.000000    0.000000     ...       \n",
       "25%      1.000000     0.000000     0.000000    0.000000     ...       \n",
       "50%      1.000000     1.000000     0.000000    0.000000     ...       \n",
       "75%      1.000000     1.000000     1.000000    0.000000     ...       \n",
       "max      1.000000     1.000000     1.000000    1.000000     ...       \n",
       "\n",
       "           higher    internet    romantic      famrel    freetime       goout  \\\n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean     0.949367    0.832911    0.334177    3.944304    3.235443    3.108861   \n",
       "std      0.219525    0.373528    0.472300    0.896659    0.998862    1.113278   \n",
       "min      0.000000    0.000000    0.000000    1.000000    1.000000    1.000000   \n",
       "25%      1.000000    1.000000    0.000000    4.000000    3.000000    2.000000   \n",
       "50%      1.000000    1.000000    0.000000    4.000000    3.000000    3.000000   \n",
       "75%      1.000000    1.000000    1.000000    5.000000    4.000000    4.000000   \n",
       "max      1.000000    1.000000    1.000000    5.000000    5.000000    5.000000   \n",
       "\n",
       "             Dalc        Walc      health    absences  \n",
       "count  395.000000  395.000000  395.000000  395.000000  \n",
       "mean     1.481013    2.291139    3.554430    5.708861  \n",
       "std      0.890741    1.287897    1.390303    8.003096  \n",
       "min      1.000000    1.000000    1.000000    0.000000  \n",
       "25%      1.000000    1.000000    3.000000    0.000000  \n",
       "50%      1.000000    2.000000    4.000000    4.000000  \n",
       "75%      2.000000    3.000000    5.000000    8.000000  \n",
       "max      5.000000    5.000000    5.000000   75.000000  \n",
       "\n",
       "[8 rows x 48 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 395 students and 30 features in the original dataset, after splitting and preprocessing, there are 300 and 95 students in training and testing datasets respectively, all of which have 48 features. The number of students are more than that of the features, so no high dimention problem. Now the features are all numerical without any missing values. Actually the feature values are all integers, some are even binary. Besides, the data is not skewed in terms of outcome, almost 2 thirds of students passed, and the rest failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 300 training samples,more than 50, sufficient for machine learning, the outcome of this problem is categorical, just two classes, \"passed\" or \"not\", and we have these labels in our dataset, so this is a supervised binary classification problem. The goal of this problem is to make predictions on whether a student pass his/her study or not. We have far less than 100,000 samples, so we can use linear SVC, ensemble classifiers such as decision tree. Also we can consider Logistic Regression which can do classification for two-class case.\n",
    "\n",
    "First, we use a **Decision Tree Model** which are commonly used in operational research, classification and regresssion for decision making.\n",
    "- Prons: It has good expressiveness, easy to understand a decision tree model and explain how each feature influences the calssifications through the nodes and branches. It is also compuationally cheap to use, missing values OK, can deal with irrelevant features. \n",
    "- Cons: for certain complex situations, i.e., if there are too many nodes to split on training data, overfitting may happen. \n",
    "\n",
    "In this case, the dataset contains many nominal features such as 'sex', 'address' and numeric features like 'age', 'absences', decison tree models can handle both types. There are not a large number of observations, just 300, which means it is easy to calculate gini or entropy for each feature and select proper nodes. Also it can work fast on prediction by just traversing the tree. Furthermore, we can explain which features are significant from the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            random_state=1, splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a model\n",
    "import time\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    print \"Training {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    print \"Done!\\nTraining time (secs): {:.3f}\".format(end - start)\n",
    "\n",
    "# TODO: Choose a model, import it and instantiate an object\n",
    "import sklearn.tree\n",
    "clf = sklearn.tree.DecisionTreeClassifier(random_state=1)#Choose a decision tree model\n",
    "\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n",
    "#print clf  # you can inspect the learned model by printing it\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for training set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    print \"Done!\\nPrediction time (secs): {:.3f}\".format(end - start)\n",
    "    return f1_score(target.values, y_pred, pos_label='yes')\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print \"F1 score for training set: {}\".format(train_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "print \"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.001\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 1.0\n",
      "node_count: 39\n",
      "max_depth: 8\n",
      "max_feature: None\n",
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.002\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for test set: 1.0\n",
      "node_count: 95\n",
      "max_depth: 12\n",
      "max_feature: None\n",
      "------------------------------------------\n",
      "Training set size: 300\n",
      "Training DecisionTreeClassifier...\n",
      "Done!\n",
      "Training time (secs): 0.002\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for training set: 1.0\n",
      "Predicting labels using DecisionTreeClassifier...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for test set: 1.0\n",
      "node_count: 129\n",
      "max_depth: 12\n",
      "max_feature: None\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using different training set sizes\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    print \"------------------------------------------\"\n",
    "    print \"Training set size: {}\".format(len(X_train))\n",
    "    train_start = time.time()\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    train_end = time.time()\n",
    "    train_time = train_end - train_start\n",
    "    train_F1 = predict_labels(clf, X_train, y_train)\n",
    "    print \"F1 score for training set: {}\".format(train_F1)\n",
    "    test_start = time.time()\n",
    "    test_F1 = predict_labels(clf, X_test, y_test)\n",
    "    print \"F1 score for test set: {}\".format(test_F1)\n",
    "    test_end = time.time()\n",
    "    test_time = test_end - test_start\n",
    "    return train_time, test_time, train_F1, test_F1\n",
    "\n",
    "# TODO: Run the helper function above for desired subsets of training data\n",
    "num_train_set = [100 ,200 ,300]\n",
    "#Create a dataframe to save training time, predicting time, training F1 score as well as testing F1 score\n",
    "time_score_table = pd.DataFrame(columns = ['model_name','train_num','train_time',\n",
    "                                           'test_time', 'train_F1', 'test_F1'])\n",
    "for i, num in enumerate(num_train_set):\n",
    "    table = train_predict(clf, X_train[:num],y_train[:num],X_test,y_test)\n",
    "    time_score_table.loc[i,'model_name'] = clf.__class__.__name__\n",
    "    time_score_table.loc[i,'train_num'] = num\n",
    "    time_score_table.loc[i,['train_time', 'test_time', 'train_F1', 'test_F1']] = table\n",
    "    #Display the complexity of trees\n",
    "    print 'node_count:',clf.tree_.node_count\n",
    "    print 'max_depth:',clf.tree_.max_depth\n",
    "    print 'max_feature:',clf.max_features\n",
    "\n",
    "# Note: Keep the test set constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>train_num</th>\n",
       "      <th>train_time</th>\n",
       "      <th>test_time</th>\n",
       "      <th>train_F1</th>\n",
       "      <th>test_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0009999275</td>\n",
       "      <td>0.0009999275</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>200</td>\n",
       "      <td>0.002000093</td>\n",
       "      <td>0.002000093</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>300</td>\n",
       "      <td>0.002000093</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name train_num    train_time     test_time train_F1  \\\n",
       "0  DecisionTreeClassifier       100  0.0009999275  0.0009999275        1   \n",
       "1  DecisionTreeClassifier       200   0.002000093   0.002000093        1   \n",
       "2  DecisionTreeClassifier       300   0.002000093             0        1   \n",
       "\n",
       "  test_F1  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_score_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows that:\n",
    "\n",
    "1. As the number of training data increases, the F1 value increases. When the number reaches 300, the F1 score of the testing dataset is over 90%.\n",
    "2. It only takes a very short time to train and predict, the training time should go up as the scale of data increases, perhaps due to random error, the time does not correspond to training scale directly.\n",
    "3. The F1 scores for the training data and testing data are over 90%, which means this model works very well without serious overfitting.\n",
    "\n",
    "Also, from the node_count, max_depth and max_feature, it seems the complexity of decision tree grows in order to fit the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.01276552  0.0171615   0.\n",
      "  0.00574449  0.01340707  0.0238018   0.00765931  0.          0.06665105\n",
      "  0.03470113  0.01128008  0.0128719   0.          0.          0.00612745\n",
      "  0.01021242  0.          0.0122549   0.00765931  0.01404208  0.01925153\n",
      "  0.01276552  0.          0.          0.02026359  0.01735397  0.0066733\n",
      "  0.01291175  0.04025101  0.10611785  0.0388254   0.01276552  0.00903599\n",
      "  0.01531863  0.01021242  0.01148897  0.          0.03486298  0.02753761\n",
      "  0.04191571  0.09310854  0.0159569   0.03389246  0.01097227  0.15217805]\n",
      "Index([u'school_GP', u'school_MS', u'sex_F', u'sex_M', u'age', u'address_R',\n",
      "       u'address_U', u'famsize_GT3', u'famsize_LE3', u'Pstatus_A',\n",
      "       u'Pstatus_T', u'Medu', u'Fedu', u'Mjob_at_home', u'Mjob_health',\n",
      "       u'Mjob_other', u'Mjob_services', u'Mjob_teacher', u'Fjob_at_home',\n",
      "       u'Fjob_health', u'Fjob_other', u'Fjob_services', u'Fjob_teacher',\n",
      "       u'reason_course', u'reason_home', u'reason_other', u'reason_reputation',\n",
      "       u'guardian_father', u'guardian_mother', u'guardian_other',\n",
      "       u'traveltime', u'studytime', u'failures', u'schoolsup', u'famsup',\n",
      "       u'paid', u'activities', u'nursery', u'higher', u'internet', u'romantic',\n",
      "       u'famrel', u'freetime', u'goout', u'Dalc', u'Walc', u'health',\n",
      "       u'absences'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print clf.feature_importances_\n",
    "print X_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems 'absences', 'goout' are quite significant in terms of prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this problem is a binary classification problem, and there are no missing values here, also the scales of features do not have huge differences, we can try **Logistic Regression Model**. Logistic regression is widely used in epidemic diseases research. It is good for binary classification. Logistic regression classifier can take features and multiply each one by a weight and then add them up, and put the result into the sigmoid. Anything above 0.5 will be classified as 1, otherwise as 0.\n",
    "- Pros: compuatationally inexpensive, easy to implement and interpret. \n",
    "- Cons: It is prone to underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression...\n",
      "Done!\n",
      "Training time (secs): 0.013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Train and predict using two other models\n",
    "#Choose Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=1)\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.061\n",
      "F1 score for training set: 0.828828828829\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.003\n",
      "F1 score for test set: 0.833333333333\n",
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training LogisticRegression...\n",
      "Done!\n",
      "Training time (secs): 0.002\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.025\n",
      "F1 score for training set: 0.881118881119\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.884057971014\n",
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training LogisticRegression...\n",
      "Done!\n",
      "Training time (secs): 0.003\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for training set: 0.831615120275\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for test set: 0.836879432624\n",
      "------------------------------------------\n",
      "Training set size: 300\n",
      "Training LogisticRegression...\n",
      "Done!\n",
      "Training time (secs): 0.003\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.019\n",
      "F1 score for training set: 0.828828828829\n",
      "Predicting labels using LogisticRegression...\n",
      "Done!\n",
      "Prediction time (secs): 0.000\n",
      "F1 score for test set: 0.833333333333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>train_num</th>\n",
       "      <th>train_time</th>\n",
       "      <th>test_time</th>\n",
       "      <th>train_F1</th>\n",
       "      <th>test_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>100</td>\n",
       "      <td>0.003000021</td>\n",
       "      <td>0.001999855</td>\n",
       "      <td>0.8811189</td>\n",
       "      <td>0.884058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>200</td>\n",
       "      <td>0.003999949</td>\n",
       "      <td>0.0009999275</td>\n",
       "      <td>0.8316151</td>\n",
       "      <td>0.8368794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>300</td>\n",
       "      <td>0.003999949</td>\n",
       "      <td>0.0009999275</td>\n",
       "      <td>0.8288288</td>\n",
       "      <td>0.8333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name train_num   train_time     test_time   train_F1  \\\n",
       "0  LogisticRegression       100  0.003000021   0.001999855  0.8811189   \n",
       "1  LogisticRegression       200  0.003999949  0.0009999275  0.8316151   \n",
       "2  LogisticRegression       300  0.003999949  0.0009999275  0.8288288   \n",
       "\n",
       "     test_F1  \n",
       "0   0.884058  \n",
       "1  0.8368794  \n",
       "2  0.8333333  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print \"F1 score for training set: {}\".format(train_f1_score)\n",
    "print \"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test))\n",
    "time_score_table = pd.DataFrame(columns = ['model_name','train_num','train_time',\n",
    "                                           'test_time', 'train_F1', 'test_F1'])\n",
    "for i, num in enumerate(num_train_set):\n",
    "    table = train_predict(clf, X_train[:num],y_train[:num],X_test,y_test)\n",
    "    time_score_table.loc[i,'model_name'] = clf.__class__.__name__\n",
    "    time_score_table.loc[i,'train_num'] = num\n",
    "    time_score_table.loc[i,['train_time', 'test_time', 'train_F1', 'test_F1']] = table\n",
    "# Note: Keep the test set constant\n",
    "time_score_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vairance of logistic regression is really small, it can prevent overfitting, but the F1 scores are lower that those of decision tree models. It takes less time to train the data, but the testing time does not look stable ,perhaps because of system random error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supporting Vector Machine** model also performs well in binary classification problem. In this case, there are 48 features, quite high dimensionï¼Œ and after feature transformation, all the features have become numeric, therefore  it is possible for the model to find some supporting vectors and create a hyperplane to separate two classes.\n",
    "\n",
    "Finally, we run a SVM which is commonly used in pattern recognition, classification and regression.\n",
    "- Pros: perform well in classification problems, especially natively binary classification(like this case). It is easy ot interpret results in terms of hyperplane, and has low generalization error. It can also be applied to non-linear cases through different kernels. \n",
    "- Cons: It is sensitive to tuning parameters and kernel choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=1,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Choose Supporting vector machine model\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(random_state=1)\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.011\n",
      "F1 score for training set: 0.874458874459\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for test set: 0.891891891892\n",
      "------------------------------------------\n",
      "Training set size: 100\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.002\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for training set: 0.890322580645\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.905405405405\n",
      "------------------------------------------\n",
      "Training set size: 200\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.004\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for training set: 0.882943143813\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.001\n",
      "F1 score for test set: 0.910344827586\n",
      "------------------------------------------\n",
      "Training set size: 300\n",
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.008\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.005\n",
      "F1 score for training set: 0.874458874459\n",
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for test set: 0.891891891892\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>train_num</th>\n",
       "      <th>train_time</th>\n",
       "      <th>test_time</th>\n",
       "      <th>train_F1</th>\n",
       "      <th>test_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001999855</td>\n",
       "      <td>0.001999855</td>\n",
       "      <td>0.8903226</td>\n",
       "      <td>0.9054054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVC</td>\n",
       "      <td>200</td>\n",
       "      <td>0.003999949</td>\n",
       "      <td>0.002000093</td>\n",
       "      <td>0.8829431</td>\n",
       "      <td>0.9103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>300</td>\n",
       "      <td>0.008000135</td>\n",
       "      <td>0.003000021</td>\n",
       "      <td>0.8744589</td>\n",
       "      <td>0.8918919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name train_num   train_time    test_time   train_F1    test_F1\n",
       "0        SVC       100  0.001999855  0.001999855  0.8903226  0.9054054\n",
       "1        SVC       200  0.003999949  0.002000093  0.8829431  0.9103448\n",
       "2        SVC       300  0.008000135  0.003000021  0.8744589  0.8918919"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print \"F1 score for training set: {}\".format(train_f1_score)\n",
    "print \"F1 score for test set: {}\".format(predict_labels(clf, X_test, y_test))\n",
    "time_score_table = pd.DataFrame(columns = ['model_name','train_num','train_time',\n",
    "                                           'test_time', 'train_F1', 'test_F1'])\n",
    "for i, num in enumerate(num_train_set):\n",
    "    table = train_predict(clf, X_train[:num],y_train[:num],X_test,y_test)\n",
    "    time_score_table.loc[i,'model_name'] = clf.__class__.__name__\n",
    "    time_score_table.loc[i,'train_num'] = num\n",
    "    time_score_table.loc[i,['train_time', 'test_time', 'train_F1', 'test_F1']] = table\n",
    "# Note: Keep the test set constant\n",
    "time_score_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last F1 score look close to that of decision tree model, better than that in logistic regression model, and the variance is also small. But the test_F1 declines in the third test, perhaps more training data makes it difficult to find a maximal margin. And it takes more time to train and test than decision tree model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choosing the Best Model\n",
    "\n",
    "- Based on the experiments you performed earlier, in 1-2 paragraphs explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?\n",
    "- In 1-2 paragraphs explain to the board of supervisors in layman's terms how the final model chosen is supposed to work (for example if you chose a Decision Tree or Support Vector Machine, how does it make a prediction).\n",
    "- Fine-tune the model. Use Gridsearch with at least one important parameter tuned and with at least 3 settings. Use the entire training set for this.\n",
    "- What is the model's final F<sub>1</sub> score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the experiments performed above, I chose decision tree as the best model, because it had good expressiveness, relatively high F1 scores than other two models on this dataset, and it was not time-consuming as SVM did. \n",
    "\n",
    "We can imagin a decision tree model as a tree in graph, each internal node represents a \"test\" on a feature, and the 'test' result decides the direction of next step. When the trained decision tree model is applied on test data, for each student the model finds a path go through the tree from the top nodes to the down leaves according to the feature values and splitting rules. In order to make it easier, here I have plotted a decision tree, and I will use this tree to illustrate how the prediction works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxkAAAG3CAYAAADYaphIAAAAAXNSR0IArs4c6QAAAARnQU1BAACx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\nDwstFyprbkk9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.Image('Rplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now we have a male student aged 20, he has 13 absenses and his school is GP. Let's make a prediction according to the tree above. The left branch always means yes, and the right one means no. First, we test the age, is he over 18?Yes. Then we move to the left branch and test his absenses, is his absences < 0.5?No. Then we move to the right branch and test absences again, is absences >=12?Yes, then we move to the left branch and test sex, is he a male?Yes, and finally we move to the left leaf which is labeled as 'no'. So this student is predicted as 'no', he will fail. For other students, we can repeat the same process and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyparsing\n",
    "import pydot\n",
    "import graphviz\n",
    "from sklearn.externals.six import StringIO\n",
    "dot_data = StringIO()\n",
    "sklearn.tree.export_graphviz(clf,out_file=dot_data)\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_pdf(\"decision_tree.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_replaced = y_train.replace(['yes', 'no'], [1, 0])\n",
    "y_test_replaced = y_test.replace(['yes', 'no'], [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            random_state=1, splitter='best'),\n",
       "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
       "       param_grid={'max_features': (5, 10, 15, 20, 30, 40, 48)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, score_func=None,\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Fine-tune your model and report the best F1 score\n",
    "clf = sklearn.tree.DecisionTreeClassifier(random_state=1)\n",
    "# Set up the parameters we wish to tune\n",
    "parameters = {'max_features':(5,10,15,20,30,40,48)}\n",
    "\n",
    "# Make an appropriate scoring function\n",
    "from sklearn.metrics import make_scorer\n",
    "scoring_function = make_scorer(f1_score,greater_is_better=True)\n",
    "\n",
    "# Make the GridSearchCV object\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "reg = GridSearchCV(clf, parameters, scoring_function) \n",
    "reg.fit(X_train,y_train_replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=40, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            random_state=1, splitter='best')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_F1 = f1_score(reg.predict(X_train),y_train_replaced)\n",
    "test_F1 = f1_score(reg.predict(X_test),y_test_replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print train_F1, test_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I tuned the max_features parameter which started from 5 to 48(the number of features), and at the point of 40, the model got optimized with a F1 score of 1, a quite good result. This meaned, it was not necessary to use all the features to train the model, 40 was sufficient. But we need more data to do cross validation on this model to check whether it is scalable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
